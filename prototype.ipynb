{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "import ray\n",
    "from is3_rl_wholesale.api.wholesale_env import Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-14 11:20:17,736 I 28584 28584] logging.cc:194: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.8.10', ray_version='1.12.1', ray_commit='4863e33856b54ccf8add5cbe75e41558850a1b75', address_info={'node_ip_address': '172.30.59.65', 'raylet_ip_address': '172.30.59.65', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-06-14_10-44-48_701551_5784/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-06-14_10-44-48_701551_5784/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2022-06-14_10-44-48_701551_5784', 'metrics_export_port': 43938, 'gcs_address': '172.30.59.65:6379', 'address': '172.30.59.65:6379', 'node_id': 'e25121e39df93e9fded332c79572c6551b889a774256a773c3cc6db3'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(address=\"auto\", namespace=\"serve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray import rllib\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30882)\u001b[0m /home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30882)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30882)\u001b[0m 2022-06-14 11:24:01,172\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30882)\u001b[0m 2022-06-14 11:24:01,172\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30882)\u001b[0m /home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/gym/spaces/box.py:142: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30882)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30882)\u001b[0m 2022-06-14 11:24:01,173\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=30882, ip=172.30.59.65, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f6ac65df550>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30882)\u001b[0m   File \"/home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 507, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30882)\u001b[0m     check_env(self.env)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30882)\u001b[0m   File \"/home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 65, in check_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30882)\u001b[0m     check_gym_environments(env)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30882)\u001b[0m   File \"/home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 139, in check_gym_environments\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30882)\u001b[0m     if not env.observation_space.contains(reset_obs):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30882)\u001b[0m   File \"/home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/gym/spaces/box.py\", line 143, in contains\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30882)\u001b[0m     x = np.asarray(x, dtype=self.dtype)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30882)\u001b[0m TypeError: float() argument must be a string or a number, not 'dict'\n"
     ]
    },
    {
     "ename": "RayActorError",
     "evalue": "The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=30880, ip=172.30.59.65, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fca70da44c0>)\n  File \"/home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 507, in __init__\n    check_env(self.env)\n  File \"/home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 65, in check_env\n    check_gym_environments(env)\n  File \"/home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 139, in check_gym_environments\n    if not env.observation_space.contains(reset_obs):\n  File \"/home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/gym/spaces/box.py\", line 143, in contains\n    x = np.asarray(x, dtype=self.dtype)\nTypeError: float() argument must be a string or a number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:896\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 896\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator)\n\u001b[1;32m    897\u001b[0m \u001b[39m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[39m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[39m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[39m# parallel to training.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[39m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n",
      "File \u001b[0;32m~/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:1035\u001b[0m, in \u001b[0;36mTrainer._init\u001b[0;34m(self, config, env_creator)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_init\u001b[39m(\u001b[39mself\u001b[39m, config: TrainerConfigDict, env_creator: EnvCreator) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1035\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/lorenz/is3-rl-wholesale/prototype.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lorenz/is3-rl-wholesale/prototype.ipynb#ch0000003vscode-remote?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m env\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lorenz/is3-rl-wholesale/prototype.ipynb#ch0000003vscode-remote?line=4'>5</a>\u001b[0m register_env(\u001b[39m\"\u001b[39m\u001b[39mmy_env\u001b[39m\u001b[39m\"\u001b[39m, env_creator)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lorenz/is3-rl-wholesale/prototype.ipynb#ch0000003vscode-remote?line=6'>7</a>\u001b[0m trainer \u001b[39m=\u001b[39m PPOTrainer(env\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmy_env\u001b[39;49m\u001b[39m\"\u001b[39;49m, config\u001b[39m=\u001b[39;49m{})\n",
      "File \u001b[0;32m~/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:830\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, config, env, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    820\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[1;32m    823\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    824\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    827\u001b[0m     }\n\u001b[1;32m    828\u001b[0m }\n\u001b[0;32m--> 830\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    831\u001b[0m     config, logger_creator, remote_checkpoint_dir, sync_function_tpl\n\u001b[1;32m    832\u001b[0m )\n",
      "File \u001b[0;32m~/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/tune/trainable.py:149\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    147\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_current_ip()\n\u001b[0;32m--> 149\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[1;32m    150\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:911\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[39m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[39m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[39m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[39m# parallel to training.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[39m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    905\u001b[0m     \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    906\u001b[0m     \u001b[39m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[39m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    910\u001b[0m     \u001b[39m# should no longer be used.\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[1;32m    912\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[1;32m    913\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[1;32m    914\u001b[0m         policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[1;32m    915\u001b[0m         trainer_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    916\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_workers\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    917\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    918\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[1;32m    919\u001b[0m     )\n\u001b[1;32m    921\u001b[0m     \u001b[39m# Function defining one single training iteration's behavior.\u001b[39;00m\n\u001b[1;32m    922\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39m_disable_execution_plan_api\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    923\u001b[0m         \u001b[39m# TODO: Ensure remote workers are initially in sync with the\u001b[39;00m\n\u001b[1;32m    924\u001b[0m         \u001b[39m# local worker.\u001b[39;00m\n\u001b[1;32m    925\u001b[0m         \u001b[39m# self.workers.sync_weights()\u001b[39;00m\n",
      "File \u001b[0;32m~/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py:134\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39m# Create a local worker, if needed.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    126\u001b[0m     local_worker\n\u001b[1;32m    127\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remote_workers\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m     )\n\u001b[1;32m    133\u001b[0m ):\n\u001b[0;32m--> 134\u001b[0m     remote_spaces \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mget(\n\u001b[1;32m    135\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mremote_workers()[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mforeach_policy\u001b[39m.\u001b[39;49mremote(\n\u001b[1;32m    136\u001b[0m             \u001b[39mlambda\u001b[39;49;00m p, pid: (pid, p\u001b[39m.\u001b[39;49mobservation_space, p\u001b[39m.\u001b[39;49maction_space)\n\u001b[1;32m    137\u001b[0m         )\n\u001b[1;32m    138\u001b[0m     )\n\u001b[1;32m    139\u001b[0m     spaces \u001b[39m=\u001b[39m {\n\u001b[1;32m    140\u001b[0m         e[\u001b[39m0\u001b[39m]: (\u001b[39mgetattr\u001b[39m(e[\u001b[39m1\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39moriginal_space\u001b[39m\u001b[39m\"\u001b[39m, e[\u001b[39m1\u001b[39m]), e[\u001b[39m2\u001b[39m])\n\u001b[1;32m    141\u001b[0m         \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m remote_spaces\n\u001b[1;32m    142\u001b[0m     }\n\u001b[1;32m    143\u001b[0m     \u001b[39m# Try to add the actual env's obs/action spaces.\u001b[39;00m\n",
      "File \u001b[0;32m~/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/worker.py:1811\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1809\u001b[0m             \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   1810\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1811\u001b[0m             \u001b[39mraise\u001b[39;00m value\n\u001b[1;32m   1813\u001b[0m \u001b[39mif\u001b[39;00m is_individual_id:\n\u001b[1;32m   1814\u001b[0m     values \u001b[39m=\u001b[39m values[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=30880, ip=172.30.59.65, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fca70da44c0>)\n  File \"/home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 507, in __init__\n    check_env(self.env)\n  File \"/home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 65, in check_env\n    check_gym_environments(env)\n  File \"/home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 139, in check_gym_environments\n    if not env.observation_space.contains(reset_obs):\n  File \"/home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/gym/spaces/box.py\", line 143, in contains\n    x = np.asarray(x, dtype=self.dtype)\nTypeError: float() argument must be a string or a number, not 'dict'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=30880)\u001b[0m /home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30880)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30880)\u001b[0m 2022-06-14 11:24:01,189\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30880)\u001b[0m 2022-06-14 11:24:01,189\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30880)\u001b[0m /home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/gym/spaces/box.py:142: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30880)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30880)\u001b[0m 2022-06-14 11:24:01,190\tERROR worker.py:449 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=30880, ip=172.30.59.65, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fca70da44c0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30880)\u001b[0m   File \"/home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 507, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30880)\u001b[0m     check_env(self.env)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30880)\u001b[0m   File \"/home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 65, in check_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30880)\u001b[0m     check_gym_environments(env)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30880)\u001b[0m   File \"/home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 139, in check_gym_environments\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30880)\u001b[0m     if not env.observation_space.contains(reset_obs):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30880)\u001b[0m   File \"/home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/gym/spaces/box.py\", line 143, in contains\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30880)\u001b[0m     x = np.asarray(x, dtype=self.dtype)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=30880)\u001b[0m TypeError: float() argument must be a string or a number, not 'dict'\n"
     ]
    }
   ],
   "source": [
    "env = Env({})\n",
    "def env_creator(env_config):\n",
    "    env = Env({})\n",
    "    return env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "trainer = PPOTrainer(env=\"my_env\", config={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 11:23:35,936\tWARNING env.py:69 -- Env checking isn't implemented for VectorEnvs, RemoteBaseEnvs, ExternalMultiAgentEnv,or ExternalEnvs or Environments that are Ray actors\n"
     ]
    }
   ],
   "source": [
    "ray.rllib.utils.check_env(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenz/is3-rl-wholesale/venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = Env({})\n",
    "TrainerConfigDict = {\n",
    "    # === Settings for Rollout Worker processes ===\n",
    "    # Number of rollout worker actors to create for parallel sampling. Setting\n",
    "    # this to 0 will force rollouts to be done in the trainer actor.\n",
    "    \"num_workers\": 2,\n",
    "    # Number of environments to evaluate vector-wise per worker. This enables\n",
    "    # model inference batching, which can improve performance for inference\n",
    "    # bottlenecked workloads.\n",
    "    \"num_envs_per_worker\": 1,\n",
    "    # When `num_workers` > 0, the driver (local_worker; worker-idx=0) does not\n",
    "    # need an environment. This is because it doesn't have to sample (done by\n",
    "    # remote_workers; worker_indices > 0) nor evaluate (done by evaluation\n",
    "    # workers; see below).\n",
    "    \"create_env_on_driver\": False,\n",
    "    # Divide episodes into fragments of this many steps each during rollouts.\n",
    "    # Sample batches of this size are collected from rollout workers and\n",
    "    # combined into a larger batch of `train_batch_size` for learning.\n",
    "    #\n",
    "    # For example, given rollout_fragment_length=100 and train_batch_size=1000:\n",
    "    #   1. RLlib collects 10 fragments of 100 steps each from rollout workers.\n",
    "    #   2. These fragments are concatenated and we perform an epoch of SGD.\n",
    "    #\n",
    "    # When using multiple envs per worker, the fragment size is multiplied by\n",
    "    # `num_envs_per_worker`. This is since we are collecting steps from\n",
    "    # multiple envs in parallel. For example, if num_envs_per_worker=5, then\n",
    "    # rollout workers will return experiences in chunks of 5*100 = 500 steps.\n",
    "    #\n",
    "    # The dataflow here can vary per algorithm. For example, PPO further\n",
    "    # divides the train batch into minibatches for multi-epoch SGD.\n",
    "    \"rollout_fragment_length\": 200,\n",
    "    # How to build per-Sampler (RolloutWorker) batches, which are then\n",
    "    # usually concat'd to form the train batch. Note that \"steps\" below can\n",
    "    # mean different things (either env- or agent-steps) and depends on the\n",
    "    # `count_steps_by` (multiagent) setting below.\n",
    "    # truncate_episodes: Each produced batch (when calling\n",
    "    #   RolloutWorker.sample()) will contain exactly `rollout_fragment_length`\n",
    "    #   steps. This mode guarantees evenly sized batches, but increases\n",
    "    #   variance as the future return must now be estimated at truncation\n",
    "    #   boundaries.\n",
    "    # complete_episodes: Each unroll happens exactly over one episode, from\n",
    "    #   beginning to end. Data collection will not stop unless the episode\n",
    "    #   terminates or a configured horizon (hard or soft) is hit.\n",
    "    \"batch_mode\": \"truncate_episodes\",\n",
    "\n",
    "    # === Settings for the Trainer process ===\n",
    "    # Discount factor of the MDP.\n",
    "    \"gamma\": 0.99,\n",
    "    # The default learning rate.\n",
    "    \"lr\": 0.0001,\n",
    "    # Training batch size, if applicable. Should be >= rollout_fragment_length.\n",
    "    # Samples batches will be concatenated together to a batch of this size,\n",
    "    # which is then passed to SGD.\n",
    "    \"train_batch_size\": 200,\n",
    "    # Arguments to pass to the policy model. See models/catalog.py for a full\n",
    "    # list of the available model options.\n",
    "    #\"model\": MODEL_DEFAULTS,\n",
    "    # Arguments to pass to the policy optimizer. These vary by optimizer.\n",
    "    \"optimizer\": {},\n",
    "\n",
    "    # === Environment Settings ===\n",
    "    # Number of steps after which the episode is forced to terminate. Defaults\n",
    "    # to `env.spec.max_episode_steps` (if present) for Gym envs.\n",
    "    \"horizon\": None,\n",
    "    # Calculate rewards but don't reset the environment when the horizon is\n",
    "    # hit. This allows value estimation and RNN state to span across logical\n",
    "    # episodes denoted by horizon. This only has an effect if horizon != inf.\n",
    "    \"soft_horizon\": False,\n",
    "    # Don't set 'done' at the end of the episode.\n",
    "    # In combination with `soft_horizon`, this works as follows:\n",
    "    # - no_done_at_end=False soft_horizon=False:\n",
    "    #   Reset env and add `done=True` at end of each episode.\n",
    "    # - no_done_at_end=True soft_horizon=False:\n",
    "    #   Reset env, but do NOT add `done=True` at end of the episode.\n",
    "    # - no_done_at_end=False soft_horizon=True:\n",
    "    #   Do NOT reset env at horizon, but add `done=True` at the horizon\n",
    "    #   (pretending the episode has terminated).\n",
    "    # - no_done_at_end=True soft_horizon=True:\n",
    "    #   Do NOT reset env at horizon and do NOT add `done=True` at the horizon.\n",
    "    \"no_done_at_end\": False,\n",
    "    # The environment specifier:\n",
    "    # This can either be a tune-registered env, via\n",
    "    # `tune.register_env([name], lambda env_ctx: [env object])`,\n",
    "    # or a string specifier of an RLlib supported type. In the latter case,\n",
    "    # RLlib will try to interpret the specifier as either an openAI gym env,\n",
    "    # a PyBullet env, a ViZDoomGym env, or a fully qualified classpath to an\n",
    "    # Env class, e.g. \"ray.rllib.examples.env.random_env.RandomEnv\".\n",
    "    \"env\": env,\n",
    "    # The observation- and action spaces for the Policies of this Trainer.\n",
    "    # Use None for automatically inferring these from the given env.\n",
    "    \"observation_space\": None,\n",
    "    \"action_space\": None,\n",
    "    # Arguments dict passed to the env creator as an EnvContext object (which\n",
    "    # is a dict plus the properties: num_workers, worker_index, vector_index,\n",
    "    # and remote).\n",
    "    \"env_config\": {},\n",
    "    # If using num_envs_per_worker > 1, whether to create those new envs in\n",
    "    # remote processes instead of in the same worker. This adds overheads, but\n",
    "    # can make sense if your envs can take much time to step / reset\n",
    "    # (e.g., for StarCraft). Use this cautiously; overheads are significant.\n",
    "    \"remote_worker_envs\": False,\n",
    "    # Timeout that remote workers are waiting when polling environments.\n",
    "    # 0 (continue when at least one env is ready) is a reasonable default,\n",
    "    # but optimal value could be obtained by measuring your environment\n",
    "    # step / reset and model inference perf.\n",
    "    \"remote_env_batch_wait_ms\": 0,\n",
    "    # A callable taking the last train results, the base env and the env\n",
    "    # context as args and returning a new task to set the env to.\n",
    "    # The env must be a `TaskSettableEnv` sub-class for this to work.\n",
    "    # See `examples/curriculum_learning.py` for an example.\n",
    "    \"env_task_fn\": None,\n",
    "    # If True, try to render the environment on the local worker or on worker\n",
    "    # 1 (if num_workers > 0). For vectorized envs, this usually means that only\n",
    "    # the first sub-environment will be rendered.\n",
    "    # In order for this to work, your env will have to implement the\n",
    "    # `render()` method which either:\n",
    "    # a) handles window generation and rendering itself (returning True) or\n",
    "    # b) returns a numpy uint8 image of shape [height x width x 3 (RGB)].\n",
    "    \"render_env\": False,\n",
    "    # If True, stores videos in this relative directory inside the default\n",
    "    # output dir (~/ray_results/...). Alternatively, you can specify an\n",
    "    # absolute path (str), in which the env recordings should be\n",
    "    # stored instead.\n",
    "    # Set to False for not recording anything.\n",
    "    # Note: This setting replaces the deprecated `monitor` key.\n",
    "    \"record_env\": False,\n",
    "    # Whether to clip rewards during Policy's postprocessing.\n",
    "    # None (default): Clip for Atari only (r=sign(r)).\n",
    "    # True: r=sign(r): Fixed rewards -1.0, 1.0, or 0.0.\n",
    "    # False: Never clip.\n",
    "    # [float value]: Clip at -value and + value.\n",
    "    # Tuple[value1, value2]: Clip at value1 and value2.\n",
    "    \"clip_rewards\": None,\n",
    "    # If True, RLlib will learn entirely inside a normalized action space\n",
    "    # (0.0 centered with small stddev; only affecting Box components).\n",
    "    # We will unsquash actions (and clip, just in case) to the bounds of\n",
    "    # the env's action space before sending actions back to the env.\n",
    "    \"normalize_actions\": True,\n",
    "    # If True, RLlib will clip actions according to the env's bounds\n",
    "    # before sending them back to the env.\n",
    "    # TODO: (sven) This option should be obsoleted and always be False.\n",
    "    \"clip_actions\": False,\n",
    "    # Whether to use \"rllib\" or \"deepmind\" preprocessors by default\n",
    "    # Set to None for using no preprocessor. In this case, the model will have\n",
    "    # to handle possibly complex observations from the environment.\n",
    "    \"preprocessor_pref\": \"deepmind\",\n",
    "\n",
    "    # === Debug Settings ===\n",
    "    # Set the ray.rllib.* log level for the agent process and its workers.\n",
    "    # Should be one of DEBUG, INFO, WARN, or ERROR. The DEBUG level will also\n",
    "    # periodically print out summaries of relevant internal dataflow (this is\n",
    "    # also printed out once at startup at the INFO level). When using the\n",
    "    # `rllib train` command, you can also use the `-v` and `-vv` flags as\n",
    "    # shorthand for INFO and DEBUG.\n",
    "    \"log_level\": \"DEBUG\",\n",
    "    # Callbacks that will be run during various phases of training. See the\n",
    "    # `DefaultCallbacks` class and `examples/custom_metrics_and_callbacks.py`\n",
    "    # for more usage information.\n",
    "    #\"callbacks\": DefaultCallbacks,\n",
    "    # Whether to attempt to continue training if a worker crashes. The number\n",
    "    # of currently healthy workers is reported as the \"num_healthy_workers\"\n",
    "    # metric.\n",
    "    \"ignore_worker_failures\": False,\n",
    "    # Whether - upon a worker failure - RLlib will try to recreate the lost worker as\n",
    "    # an identical copy of the failed one. The new worker will only differ from the\n",
    "    # failed one in its `self.recreated_worker=True` property value. It will have\n",
    "    # the same `worker_index` as the original one.\n",
    "    # If True, the `ignore_worker_failures` setting will be ignored.\n",
    "    \"recreate_failed_workers\": False,\n",
    "    # Log system resource metrics to results. This requires `psutil` to be\n",
    "    # installed for sys stats, and `gputil` for GPU metrics.\n",
    "    \"log_sys_usage\": True,\n",
    "    # Use fake (infinite speed) sampler. For testing only.\n",
    "    \"fake_sampler\": False,\n",
    "\n",
    "    # === Deep Learning Framework Settings ===\n",
    "    # tf: TensorFlow (static-graph)\n",
    "    # tf2: TensorFlow 2.x (eager or traced, if eager_tracing=True)\n",
    "    # tfe: TensorFlow eager (or traced, if eager_tracing=True)\n",
    "    # torch: PyTorch\n",
    "    \"framework\": \"tf\",\n",
    "    # Enable tracing in eager mode. This greatly improves performance\n",
    "    # (speedup ~2x), but makes it slightly harder to debug since Python\n",
    "    # code won't be evaluated after the initial eager pass.\n",
    "    # Only possible if framework=[tf2|tfe].\n",
    "    \"eager_tracing\": False,\n",
    "    # Maximum number of tf.function re-traces before a runtime error is raised.\n",
    "    # This is to prevent unnoticed retraces of methods inside the\n",
    "    # `..._eager_traced` Policy, which could slow down execution by a\n",
    "    # factor of 4, without the user noticing what the root cause for this\n",
    "    # slowdown could be.\n",
    "    # Only necessary for framework=[tf2|tfe].\n",
    "    # Set to None to ignore the re-trace count and never throw an error.\n",
    "    \"eager_max_retraces\": 20,\n",
    "\n",
    "    # === Exploration Settings ===\n",
    "    # Default exploration behavior, iff `explore`=None is passed into\n",
    "    # compute_action(s).\n",
    "    # Set to False for no exploration behavior (e.g., for evaluation).\n",
    "    \"explore\": True,\n",
    "    # Provide a dict specifying the Exploration object's config.\n",
    "    \"exploration_config\": {\n",
    "        # The Exploration class to use. In the simplest case, this is the name\n",
    "        # (str) of any class present in the `rllib.utils.exploration` package.\n",
    "        # You can also provide the python class directly or the full location\n",
    "        # of your class (e.g. \"ray.rllib.utils.exploration.epsilon_greedy.\n",
    "        # EpsilonGreedy\").\n",
    "        \"type\": \"StochasticSampling\",\n",
    "        # Add constructor kwargs here (if any).\n",
    "    },\n",
    "    # === Evaluation Settings ===\n",
    "    # Evaluate with every `evaluation_interval` training iterations.\n",
    "    # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "    # Note that for Ape-X metrics are already only reported for the lowest\n",
    "    # epsilon workers (least random workers).\n",
    "    # Set to None (or 0) for no evaluation.\n",
    "    \"evaluation_interval\": None,\n",
    "    # Duration for which to run evaluation each `evaluation_interval`.\n",
    "    # The unit for the duration can be set via `evaluation_duration_unit` to\n",
    "    # either \"episodes\" (default) or \"timesteps\".\n",
    "    # If using multiple evaluation workers (evaluation_num_workers > 1),\n",
    "    # the load to run will be split amongst these.\n",
    "    # If the value is \"auto\":\n",
    "    # - For `evaluation_parallel_to_training=True`: Will run as many\n",
    "    #   episodes/timesteps that fit into the (parallel) training step.\n",
    "    # - For `evaluation_parallel_to_training=False`: Error.\n",
    "    \"evaluation_duration\": 10,\n",
    "    # The unit, with which to count the evaluation duration. Either \"episodes\"\n",
    "    # (default) or \"timesteps\".\n",
    "    \"evaluation_duration_unit\": \"episodes\",\n",
    "    # Whether to run evaluation in parallel to a Trainer.train() call\n",
    "    # using threading. Default=False.\n",
    "    # E.g. evaluation_interval=2 -> For every other training iteration,\n",
    "    # the Trainer.train() and Trainer.evaluate() calls run in parallel.\n",
    "    # Note: This is experimental. Possible pitfalls could be race conditions\n",
    "    # for weight synching at the beginning of the evaluation loop.\n",
    "    \"evaluation_parallel_to_training\": False,\n",
    "    # Internal flag that is set to True for evaluation workers.\n",
    "    \"in_evaluation\": False,\n",
    "    # Typical usage is to pass extra args to evaluation env creator\n",
    "    # and to disable exploration by computing deterministic actions.\n",
    "    # IMPORTANT NOTE: Policy gradient algorithms are able to find the optimal\n",
    "    # policy, even if this is a stochastic one. Setting \"explore=False\" here\n",
    "    # will result in the evaluation workers not using this optimal policy!\n",
    "    \"evaluation_config\": {\n",
    "        # Example: overriding env_config, exploration, etc:\n",
    "        # \"env_config\": {...},\n",
    "        # \"explore\": False\n",
    "    },\n",
    "\n",
    "    # === Replay Buffer Settings ===\n",
    "    # Provide a dict specifying the ReplayBuffer's config.\n",
    "    # \"replay_buffer_config\": {\n",
    "    #     The ReplayBuffer class to use. Any class that obeys the\n",
    "    #     ReplayBuffer API can be used here. In the simplest case, this is the\n",
    "    #     name (str) of any class present in the `rllib.utils.replay_buffers`\n",
    "    #     package. You can also provide the python class directly or the\n",
    "    #     full location of your class (e.g.\n",
    "    #     \"ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer\").\n",
    "    #     \"type\": \"ReplayBuffer\",\n",
    "    #     The capacity of units that can be stored in one ReplayBuffer\n",
    "    #     instance before eviction.\n",
    "    #     \"capacity\": 10000,\n",
    "    #     Specifies how experiences are stored. Either 'sequences' or\n",
    "    #     'timesteps'.\n",
    "    #     \"storage_unit\": \"timesteps\",\n",
    "    #     Add constructor kwargs here (if any).\n",
    "    # },\n",
    "\n",
    "    # Number of parallel workers to use for evaluation. Note that this is set\n",
    "    # to zero by default, which means evaluation will be run in the trainer\n",
    "    # process (only if evaluation_interval is not None). If you increase this,\n",
    "    # it will increase the Ray resource usage of the trainer since evaluation\n",
    "    # workers are created separately from rollout workers (used to sample data\n",
    "    # for training).\n",
    "    \"evaluation_num_workers\": 0,\n",
    "    # Customize the evaluation method. This must be a function of signature\n",
    "    # (trainer: Trainer, eval_workers: WorkerSet) -> metrics: dict. See the\n",
    "    # Trainer.evaluate() method to see the default implementation.\n",
    "    # The Trainer guarantees all eval workers have the latest policy state\n",
    "    # before this function is called.\n",
    "    \"custom_eval_function\": None,\n",
    "    # Make sure the latest available evaluation results are always attached to\n",
    "    # a step result dict.\n",
    "    # This may be useful if Tune or some other meta controller needs access\n",
    "    # to evaluation metrics all the time.\n",
    "    \"always_attach_evaluation_results\": False,\n",
    "    # Store raw custom metrics without calculating max, min, mean\n",
    "    \"keep_per_episode_custom_metrics\": False,\n",
    "\n",
    "    # === Advanced Rollout Settings ===\n",
    "    # Use a background thread for sampling (slightly off-policy, usually not\n",
    "    # advisable to turn on unless your env specifically requires it).\n",
    "    \"sample_async\": False,\n",
    "\n",
    "    # The SampleCollector class to be used to collect and retrieve\n",
    "    # environment-, model-, and sampler data. Override the SampleCollector base\n",
    "    # class to implement your own collection/buffering/retrieval logic.\n",
    "    #\"sample_collector\": SimpleListCollector,\n",
    "\n",
    "    # Element-wise observation filter, either \"NoFilter\" or \"MeanStdFilter\".\n",
    "    \"observation_filter\": \"NoFilter\",\n",
    "    # Whether to synchronize the statistics of remote filters.\n",
    "    \"synchronize_filters\": True,\n",
    "    # Configures TF for single-process operation by default.\n",
    "    \"tf_session_args\": {\n",
    "        # note: overridden by `local_tf_session_args`\n",
    "        \"intra_op_parallelism_threads\": 2,\n",
    "        \"inter_op_parallelism_threads\": 2,\n",
    "        \"gpu_options\": {\n",
    "            \"allow_growth\": True,\n",
    "        },\n",
    "        \"log_device_placement\": False,\n",
    "        \"device_count\": {\n",
    "            \"CPU\": 1\n",
    "        },\n",
    "        # Required by multi-GPU (num_gpus > 1).\n",
    "        \"allow_soft_placement\": True,\n",
    "    },\n",
    "    # Override the following tf session args on the local worker\n",
    "    \"local_tf_session_args\": {\n",
    "        # Allow a higher level of parallelism by default, but not unlimited\n",
    "        # since that can cause crashes with many concurrent drivers.\n",
    "        \"intra_op_parallelism_threads\": 8,\n",
    "        \"inter_op_parallelism_threads\": 8,\n",
    "    },\n",
    "    # Whether to LZ4 compress individual observations.\n",
    "    \"compress_observations\": False,\n",
    "    # Wait for metric batches for at most this many seconds. Those that\n",
    "    # have not returned in time will be collected in the next train iteration.\n",
    "    \"metrics_episode_collection_timeout_s\": 180,\n",
    "    # Smooth metrics over this many episodes.\n",
    "    \"metrics_num_episodes_for_smoothing\": 100,\n",
    "    # Minimum time interval to run one `train()` call for:\n",
    "    # If - after one `step_attempt()`, this time limit has not been reached,\n",
    "    # will perform n more `step_attempt()` calls until this minimum time has\n",
    "    # been consumed. Set to None or 0 for no minimum time.\n",
    "    \"min_time_s_per_reporting\": None,\n",
    "    # Minimum train/sample timesteps to optimize for per `train()` call.\n",
    "    # This value does not affect learning, only the length of train iterations.\n",
    "    # If - after one `step_attempt()`, the timestep counts (sampling or\n",
    "    # training) have not been reached, will perform n more `step_attempt()`\n",
    "    # calls until the minimum timesteps have been executed.\n",
    "    # Set to None or 0 for no minimum timesteps.\n",
    "    \"min_train_timesteps_per_reporting\": None,\n",
    "    \"min_sample_timesteps_per_reporting\": None,\n",
    "\n",
    "    # This argument, in conjunction with worker_index, sets the random seed of\n",
    "    # each worker, so that identically configured trials will have identical\n",
    "    # results. This makes experiments reproducible.\n",
    "    \"seed\": None,\n",
    "    # Any extra python env vars to set in the trainer process, e.g.,\n",
    "    # {\"OMP_NUM_THREADS\": \"16\"}\n",
    "    \"extra_python_environs_for_driver\": {},\n",
    "    # The extra python environments need to set for worker processes.\n",
    "    \"extra_python_environs_for_worker\": {},\n",
    "\n",
    "    # === Resource Settings ===\n",
    "    # Number of GPUs to allocate to the trainer process. Note that not all\n",
    "    # algorithms can take advantage of trainer GPUs. Support for multi-GPU\n",
    "    # is currently only available for tf-[PPO/IMPALA/DQN/PG].\n",
    "    # This can be fractional (e.g., 0.3 GPUs).\n",
    "    \"num_gpus\": 0,\n",
    "    # Set to True for debugging (multi-)?GPU funcitonality on a CPU machine.\n",
    "    # GPU towers will be simulated by graphs located on CPUs in this case.\n",
    "    # Use `num_gpus` to test for different numbers of fake GPUs.\n",
    "    \"_fake_gpus\": False,\n",
    "    # Number of CPUs to allocate per worker.\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "    # Number of GPUs to allocate per worker. This can be fractional. This is\n",
    "    # usually needed only if your env itself requires a GPU (i.e., it is a\n",
    "    # GPU-intensive video game), or model inference is unusually expensive.\n",
    "    \"num_gpus_per_worker\": 0,\n",
    "    # Any custom Ray resources to allocate per worker.\n",
    "    \"custom_resources_per_worker\": {},\n",
    "    # Number of CPUs to allocate for the trainer. Note: this only takes effect\n",
    "    # when running in Tune. Otherwise, the trainer runs in the main program.\n",
    "    \"num_cpus_for_driver\": 1,\n",
    "    # The strategy for the placement group factory returned by\n",
    "    # `Trainer.default_resource_request()`. A PlacementGroup defines, which\n",
    "    # devices (resources) should always be co-located on the same node.\n",
    "    # For example, a Trainer with 2 rollout workers, running with\n",
    "    # num_gpus=1 will request a placement group with the bundles:\n",
    "    # [{\"gpu\": 1, \"cpu\": 1}, {\"cpu\": 1}, {\"cpu\": 1}], where the first bundle is\n",
    "    # for the driver and the other 2 bundles are for the two workers.\n",
    "    # These bundles can now be \"placed\" on the same or different\n",
    "    # nodes depending on the value of `placement_strategy`:\n",
    "    # \"PACK\": Packs bundles into as few nodes as possible.\n",
    "    # \"SPREAD\": Places bundles across distinct nodes as even as possible.\n",
    "    # \"STRICT_PACK\": Packs bundles into one node. The group is not allowed\n",
    "    #   to span multiple nodes.\n",
    "    # \"STRICT_SPREAD\": Packs bundles across distinct nodes.\n",
    "    \"placement_strategy\": \"PACK\",\n",
    "\n",
    "    # TODO(jungong, sven): we can potentially unify all input types\n",
    "    #     under input and input_config keys. E.g.\n",
    "    #     input: sample\n",
    "    #     input_config {\n",
    "    #         env: Cartpole-v0\n",
    "    #     }\n",
    "    #     or:\n",
    "    #     input: json_reader\n",
    "    #     input_config {\n",
    "    #         path: /tmp/\n",
    "    #     }\n",
    "    #     or:\n",
    "    #     input: dataset\n",
    "    #     input_config {\n",
    "    #         format: parquet\n",
    "    #         path: /tmp/\n",
    "    #     }\n",
    "    # === Offline Datasets ===\n",
    "    # Specify how to generate experiences:\n",
    "    #  - \"sampler\": Generate experiences via online (env) simulation (default).\n",
    "    #  - A local directory or file glob expression (e.g., \"/tmp/*.json\").\n",
    "    #  - A list of individual file paths/URIs (e.g., [\"/tmp/1.json\",\n",
    "    #    \"s3://bucket/2.json\"]).\n",
    "    #  - A dict with string keys and sampling probabilities as values (e.g.,\n",
    "    #    {\"sampler\": 0.4, \"/tmp/*.json\": 0.4, \"s3://bucket/expert.json\": 0.2}).\n",
    "    #  - A callable that takes an `IOContext` object as only arg and returns a\n",
    "    #    ray.rllib.offline.InputReader.\n",
    "    #  - A string key that indexes a callable with tune.registry.register_input\n",
    "    \"input\": \"sampler\",\n",
    "    # Arguments accessible from the IOContext for configuring custom input\n",
    "    \"input_config\": {},\n",
    "    # True, if the actions in a given offline \"input\" are already normalized\n",
    "    # (between -1.0 and 1.0). This is usually the case when the offline\n",
    "    # file has been generated by another RLlib algorithm (e.g. PPO or SAC),\n",
    "    # while \"normalize_actions\" was set to True.\n",
    "    \"actions_in_input_normalized\": False,\n",
    "    # Specify how to evaluate the current policy. This only has an effect when\n",
    "    # reading offline experiences (\"input\" is not \"sampler\").\n",
    "    # Available options:\n",
    "    #  - \"wis\": the weighted step-wise importance sampling estimator.\n",
    "    #  - \"is\": the step-wise importance sampling estimator.\n",
    "    #  - \"simulation\": run the environment in the background, but use\n",
    "    #    this data for evaluation only and not for learning.\n",
    "    \"input_evaluation\": [\"is\", \"wis\"],\n",
    "    # Whether to run postprocess_trajectory() on the trajectory fragments from\n",
    "    # offline inputs. Note that postprocessing will be done using the *current*\n",
    "    # policy, not the *behavior* policy, which is typically undesirable for\n",
    "    # on-policy algorithms.\n",
    "    \"postprocess_inputs\": False,\n",
    "    # If positive, input batches will be shuffled via a sliding window buffer\n",
    "    # of this number of batches. Use this if the input data is not in random\n",
    "    # enough order. Input is delayed until the shuffle buffer is filled.\n",
    "    \"shuffle_buffer_size\": 0,\n",
    "    # Specify where experiences should be saved:\n",
    "    #  - None: don't save any experiences\n",
    "    #  - \"logdir\" to save to the agent log dir\n",
    "    #  - a path/URI to save to a custom output directory (e.g., \"s3://bucket/\")\n",
    "    #  - a function that returns a rllib.offline.OutputWriter\n",
    "    \"output\": None,\n",
    "    # Arguments accessible from the IOContext for configuring custom output\n",
    "    \"output_config\": {},\n",
    "    # What sample batch columns to LZ4 compress in the output data.\n",
    "    \"output_compress_columns\": [\"obs\", \"new_obs\"],\n",
    "    # Max output file size (in bytes) before rolling over to a new file.\n",
    "    \"output_max_file_size\": 64 * 1024 * 1024,\n",
    "\n",
    "    # === Settings for Multi-Agent Environments ===\n",
    "    \"multiagent\": {\n",
    "        # Map of type MultiAgentPolicyConfigDict from policy ids to tuples\n",
    "        # of (policy_cls, obs_space, act_space, config). This defines the\n",
    "        # observation and action spaces of the policies and any extra config.\n",
    "        \"policies\": {},\n",
    "        # Keep this many policies in the \"policy_map\" (before writing\n",
    "        # least-recently used ones to disk/S3).\n",
    "        \"policy_map_capacity\": 100,\n",
    "        # Where to store overflowing (least-recently used) policies?\n",
    "        # Could be a directory (str) or an S3 location. None for using\n",
    "        # the default output dir.\n",
    "        \"policy_map_cache\": None,\n",
    "        # Function mapping agent ids to policy ids.\n",
    "        \"policy_mapping_fn\": None,\n",
    "        # Determines those policies that should be updated.\n",
    "        # Options are:\n",
    "        # - None, for all policies.\n",
    "        # - An iterable of PolicyIDs that should be updated.\n",
    "        # - A callable, taking a PolicyID and a SampleBatch or MultiAgentBatch\n",
    "        #   and returning a bool (indicating whether the given policy is trainable\n",
    "        #   or not, given the particular batch). This allows you to have a policy\n",
    "        #   trained only on certain data (e.g. when playing against a certain\n",
    "        #   opponent).\n",
    "        \"policies_to_train\": None,\n",
    "        # Optional function that can be used to enhance the local agent\n",
    "        # observations to include more state.\n",
    "        # See rllib/evaluation/observation_function.py for more info.\n",
    "        \"observation_fn\": None,\n",
    "        # When replay_mode=lockstep, RLlib will replay all the agent\n",
    "        # transitions at a particular timestep together in a batch. This allows\n",
    "        # the policy to implement differentiable shared computations between\n",
    "        # agents it controls at that timestep. When replay_mode=independent,\n",
    "        # transitions are replayed independently per policy.\n",
    "        \"replay_mode\": \"independent\",\n",
    "        # Which metric to use as the \"batch size\" when building a\n",
    "        # MultiAgentBatch. The two supported values are:\n",
    "        # env_steps: Count each time the env is \"stepped\" (no matter how many\n",
    "        #   multi-agent actions are passed/how many multi-agent observations\n",
    "        #   have been returned in the previous step).\n",
    "        # agent_steps: Count each individual agent step as one step.\n",
    "        \"count_steps_by\": \"env_steps\",\n",
    "    },\n",
    "\n",
    "    # === Logger ===\n",
    "    # Define logger-specific configuration to be used inside Logger\n",
    "    # Default value None allows overwriting with nested dicts\n",
    "    \"logger_config\": None,\n",
    "\n",
    "    # === API deprecations/simplifications/changes ===\n",
    "    # Experimental flag.\n",
    "    # If True, TFPolicy will handle more than one loss/optimizer.\n",
    "    # Set this to True, if you would like to return more than\n",
    "    # one loss term from your `loss_fn` and an equal number of optimizers\n",
    "    # from your `optimizer_fn`.\n",
    "    # In the future, the default for this will be True.\n",
    "    \"_tf_policy_handles_more_than_one_loss\": False,\n",
    "    # Experimental flag.\n",
    "    # If True, no (observation) preprocessor will be created and\n",
    "    # observations will arrive in model as they are returned by the env.\n",
    "    # In the future, the default for this will be True.\n",
    "    \"_disable_preprocessor_api\": False,\n",
    "    # Experimental flag.\n",
    "    # If True, RLlib will no longer flatten the policy-computed actions into\n",
    "    # a single tensor (for storage in SampleCollectors/output files/etc..),\n",
    "    # but leave (possibly nested) actions as-is. Disabling flattening affects:\n",
    "    # - SampleCollectors: Have to store possibly nested action structs.\n",
    "    # - Models that have the previous action(s) as part of their input.\n",
    "    # - Algorithms reading from offline files (incl. action information).\n",
    "    \"_disable_action_flattening\": False,\n",
    "    # Experimental flag.\n",
    "    # If True, the execution plan API will not be used. Instead,\n",
    "    # a Trainer's `training_iteration` method will be called as-is each\n",
    "    # training iteration.\n",
    "    \"_disable_execution_plan_api\": False,\n",
    "\n",
    "    # If True, disable the environment pre-checking module.\n",
    "    \"disable_env_checking\": False,\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [404]>\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\"http://127.0.0.1:8000/wholesale\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"test\": \"test2\"\\n}'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test4'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1.get(\"test3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "062e5ca89d65d5206a44b988348f4da8b02334087c3e87998b1375d59c3ad2b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
